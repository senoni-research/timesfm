{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# TimesFM 2.5 + Covariates for VN2 Inventory Forecasting\n",
        "\n",
        "This notebook tests TimesFM 2.5 with covariates on the VN2 inventory planning challenge data.\n",
        "\n",
        "## Approach\n",
        "\n",
        "1. Load sales data from `../../vn2inventory/data`\n",
        "2. Prepare features and covariates\n",
        "3. Run forecasts:\n",
        "   - Baseline: TimesFM without covariates\n",
        "   - Enhanced: TimesFM with covariates\n",
        "4. Evaluate using WPMAPE and MAE+BIAS\n",
        "5. Compare results\n",
        "\n",
        "## Evaluation Metrics\n",
        "\n",
        "- **WPMAPE**: Weighted Mean Absolute Percentage Error\n",
        "- **MAE**: Mean Absolute Error  \n",
        "- **BIAS**: Mean Bias (signed error to detect over/under forecasting)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import timesfm\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "sns.set_theme(style=\"whitegrid\")\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load VN2 Inventory Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data paths (relative to timesfm/notebooks/)\n",
        "DATA_DIR = Path(\"../../vn2inventory/data\").resolve()\n",
        "\n",
        "# Load datasets\n",
        "sales_wide = pd.read_csv(DATA_DIR / \"Week 0 - 2024-04-08 - Sales.csv\")\n",
        "avail_wide = pd.read_csv(DATA_DIR / \"Week 0 - In Stock.csv\")\n",
        "master_df = pd.read_csv(DATA_DIR / \"Week 0 - Master.csv\")\n",
        "init_df = pd.read_csv(DATA_DIR / \"Week 0 - 2024-04-08 - Initial State.csv\")\n",
        "\n",
        "print(f\"Sales data shape: {sales_wide.shape}\")\n",
        "print(f\"Products: {len(sales_wide)}\")\n",
        "print(f\"Weeks of history: {sales_wide.shape[1] - 2}\")\n",
        "\n",
        "sales_wide.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Reshape and Prepare Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Melt to long format\n",
        "id_cols = [\"Store\", \"Product\"]\n",
        "week_cols = [col for col in sales_wide.columns if col not in id_cols]\n",
        "\n",
        "sales_long = sales_wide.melt(\n",
        "    id_vars=id_cols,\n",
        "    value_vars=week_cols,\n",
        "    var_name=\"date\",\n",
        "    value_name=\"sales_qty\"\n",
        ")\n",
        "\n",
        "# Convert dates and add temporal features\n",
        "sales_long[\"date\"] = pd.to_datetime(sales_long[\"date\"])\n",
        "sales_long[\"week_of_year\"] = sales_long[\"date\"].dt.isocalendar().week\n",
        "sales_long[\"month\"] = sales_long[\"date\"].dt.month\n",
        "sales_long[\"quarter\"] = sales_long[\"date\"].dt.quarter\n",
        "sales_long[\"year\"] = sales_long[\"date\"].dt.year\n",
        "\n",
        "# Coerce sales to numeric\n",
        "sales_long[\"sales_qty\"] = pd.to_numeric(sales_long[\"sales_qty\"], errors=\"coerce\").fillna(0.0)\n",
        "\n",
        "# Sort by store, product, date\n",
        "sales_long = sales_long.sort_values([\"Store\", \"Product\", \"date\"]).reset_index(drop=True)\n",
        "\n",
        "print(f\"Long format shape: {sales_long.shape}\")\n",
        "print(f\"Date range: {sales_long['date'].min()} to {sales_long['date'].max()}\")\n",
        "print(f\"Total weeks: {len(week_cols)}\")\n",
        "\n",
        "# Add week index (0, 1, 2, ...)\n",
        "min_date = sales_long[\"date\"].min()\n",
        "sales_long[\"week_index\"] = ((sales_long[\"date\"] - min_date).dt.days // 7).astype(int)\n",
        "\n",
        "sales_long.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train/Test Split\n",
        "\n",
        "We'll use:\n",
        "- **Context**: First 140 weeks for training\n",
        "- **Validation**: Last 16 weeks for evaluation  \n",
        "- **Horizon**: 3 weeks ahead (P=3: lead time + review period)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "CONTEXT_LENGTH = 140  # Use 140 weeks for context\n",
        "HORIZON = 3  # Forecast 3 weeks ahead\n",
        "VALIDATION_WEEKS = 16  # Hold out last 16 weeks for validation\n",
        "TEST_START_WEEK = 140  # Start validation from week 140\n",
        "\n",
        "# Get unique SKUs\n",
        "sku_list = sales_long[[\"Store\", \"Product\"]].drop_duplicates().reset_index(drop=True)\n",
        "print(f\"Total SKUs: {len(sku_list)}\")\n",
        "\n",
        "# For faster testing, use a subset\n",
        "USE_SUBSET = True\n",
        "SUBSET_SIZE = 50\n",
        "\n",
        "if USE_SUBSET:\n",
        "    # Select top SKUs by total volume\n",
        "    sku_volumes = sales_long.groupby([\"Store\", \"Product\"])[\"sales_qty\"].sum().reset_index()\n",
        "    sku_volumes = sku_volumes.sort_values(\"sales_qty\", ascending=False)\n",
        "    sku_list = sku_volumes.head(SUBSET_SIZE)[[\"Store\", \"Product\"]]\n",
        "    print(f\"Using subset of {len(sku_list)} high-volume SKUs for testing\")\n",
        "\n",
        "sku_list.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Forecasting Inputs and Covariates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare inputs and covariates\n",
        "inputs = []\n",
        "actuals = []  # For validation\n",
        "static_cat = {\"store\": [], \"product_group\": [], \"department\": []}\n",
        "static_num = {\"mean_demand\": [], \"cv_demand\": []}\n",
        "dynamic_cat = {\"month\": [], \"week_of_year\": [], \"quarter\": []}\n",
        "dynamic_num = {\"week_index\": []}\n",
        "\n",
        "print(\"Preparing data...\")\n",
        "for idx, row in sku_list.iterrows():\n",
        "    store, product = row[\"Store\"], row[\"Product\"]\n",
        "    \n",
        "    # Get sales history for this SKU\n",
        "    sku_data = sales_long[\n",
        "        (sales_long[\"Store\"] == store) & \n",
        "        (sales_long[\"Product\"] == product)\n",
        "    ].sort_values(\"date\").reset_index(drop=True)\n",
        "    \n",
        "    if len(sku_data) < TEST_START_WEEK + HORIZON:\n",
        "        continue  # Skip SKUs with insufficient data\n",
        "    \n",
        "    # Split into context and validation\n",
        "    context_data = sku_data.iloc[:TEST_START_WEEK].copy()\n",
        "    validation_data = sku_data.iloc[TEST_START_WEEK:TEST_START_WEEK + HORIZON].copy()\n",
        "    \n",
        "    # Use last CONTEXT_LENGTH weeks for training\n",
        "    train_data = context_data.iloc[-CONTEXT_LENGTH:].copy()\n",
        "    \n",
        "    # Sales history (inputs)\n",
        "    inputs.append(train_data[\"sales_qty\"].values)\n",
        "    \n",
        "    # Actuals for validation\n",
        "    actuals.append(validation_data[\"sales_qty\"].values)\n",
        "    \n",
        "    # Static covariates from master data\n",
        "    master_row = master_df[\n",
        "        (master_df[\"Store\"] == store) & \n",
        "        (master_df[\"Product\"] == product)\n",
        "    ]\n",
        "    \n",
        "    if len(master_row) == 0:\n",
        "        # If not found, use defaults\n",
        "        static_cat[\"store\"].append(store)\n",
        "        static_cat[\"product_group\"].append(0)\n",
        "        static_cat[\"department\"].append(0)\n",
        "    else:\n",
        "        master_row = master_row.iloc[0]\n",
        "        static_cat[\"store\"].append(store)\n",
        "        static_cat[\"product_group\"].append(master_row[\"ProductGroup\"])\n",
        "        static_cat[\"department\"].append(master_row[\"Department\"])\n",
        "    \n",
        "    # Static numerical: historical demand stats\n",
        "    mean_demand = train_data[\"sales_qty\"].mean()\n",
        "    std_demand = train_data[\"sales_qty\"].std()\n",
        "    cv_demand = std_demand / mean_demand if mean_demand > 0 else 0\n",
        "    static_num[\"mean_demand\"].append(mean_demand)\n",
        "    static_num[\"cv_demand\"].append(cv_demand)\n",
        "    \n",
        "    # Dynamic covariates (context + horizon)\n",
        "    # For future, we extend the temporal features\n",
        "    future_dates = pd.date_range(\n",
        "        train_data[\"date\"].iloc[-1] + pd.Timedelta(weeks=1),\n",
        "        periods=HORIZON,\n",
        "        freq=\"W\"\n",
        "    )\n",
        "    \n",
        "    all_dates = pd.concat([\n",
        "        train_data[\"date\"],\n",
        "        pd.Series(future_dates)\n",
        "    ]).reset_index(drop=True)\n",
        "    \n",
        "    # Month\n",
        "    months = [pd.Timestamp(d).month for d in all_dates]\n",
        "    dynamic_cat[\"month\"].append(months)\n",
        "    \n",
        "    # Week of year\n",
        "    weeks = [pd.Timestamp(d).isocalendar().week for d in all_dates]\n",
        "    dynamic_cat[\"week_of_year\"].append(weeks)\n",
        "    \n",
        "    # Quarter\n",
        "    quarters = [pd.Timestamp(d).quarter for d in all_dates]\n",
        "    dynamic_cat[\"quarter\"].append(quarters)\n",
        "    \n",
        "    # Week index\n",
        "    week_indices = list(train_data[\"week_index\"].values) + list(range(\n",
        "        train_data[\"week_index\"].iloc[-1] + 1,\n",
        "        train_data[\"week_index\"].iloc[-1] + 1 + HORIZON\n",
        "    ))\n",
        "    dynamic_num[\"week_index\"].append(week_indices)\n",
        "\n",
        "print(f\"Prepared {len(inputs)} SKUs for forecasting\")\n",
        "print(f\"Context length: {CONTEXT_LENGTH} weeks\")\n",
        "print(f\"Forecast horizon: {HORIZON} weeks\")\n",
        "print(f\"Input shape example: {inputs[0].shape}\")\n",
        "print(f\"Actuals shape example: {actuals[0].shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Load and Compile TimesFM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading TimesFM 2.5 model...\")\n",
        "model = timesfm.TimesFM_2p5_200M_torch.from_pretrained(\"google/timesfm-2.5-200m-pytorch\")\n",
        "\n",
        "print(\"Compiling model...\")\n",
        "model.compile(\n",
        "    timesfm.ForecastConfig(\n",
        "        max_context=512,\n",
        "        max_horizon=128,\n",
        "        normalize_inputs=True,\n",
        "        use_continuous_quantile_head=True,\n",
        "        force_flip_invariance=True,\n",
        "        infer_is_positive=True,\n",
        "        fix_quantile_crossing=True,\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Model ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Generate Forecasts\n",
        "\n",
        "### 6.1 Baseline: TimesFM without Covariates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print(\"Running baseline forecast (TimesFM without covariates)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "point_forecast_baseline, quantile_forecast_baseline = model.forecast(\n",
        "    horizon=HORIZON,\n",
        "    inputs=inputs,\n",
        ")\n",
        "\n",
        "baseline_time = time.time() - start_time\n",
        "print(f\"Baseline forecast completed in {baseline_time:.2f} seconds\")\n",
        "print(f\"Forecast shape: {point_forecast_baseline.shape}\")\n",
        "\n",
        "point_forecast_baseline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Enhanced: TimesFM with Covariates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running enhanced forecast (TimesFM with covariates)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "cov_forecast, xreg_forecast = model.forecast_with_covariates(\n",
        "    horizon=HORIZON,\n",
        "    inputs=inputs,\n",
        "    static_categorical_covariates=static_cat,\n",
        "    static_numerical_covariates=static_num,\n",
        "    dynamic_categorical_covariates=dynamic_cat,\n",
        "    dynamic_numerical_covariates=dynamic_num,\n",
        "    xreg_mode=\"xreg + timesfm\",  # Fit covariates first, then forecast residuals\n",
        "    normalize_xreg_target_per_input=True,\n",
        "    ridge=0.01,  # Small ridge penalty for stability\n",
        "    force_on_cpu=False,\n",
        ")\n",
        "\n",
        "cov_time = time.time() - start_time\n",
        "print(f\"Covariate forecast completed in {cov_time:.2f} seconds\")\n",
        "print(f\"Forecast shape: {len(cov_forecast)} SKUs √ó {HORIZON} weeks\")\n",
        "\n",
        "# Convert list to array for easier handling\n",
        "cov_forecast_array = np.array([f for f in cov_forecast])\n",
        "xreg_forecast_array = np.array([f for f in xreg_forecast])\n",
        "\n",
        "print(f\"\\\\nCovariates forecast shape: {cov_forecast_array.shape}\")\n",
        "print(f\"XReg only forecast shape: {xreg_forecast_array.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation Metrics\n",
        "\n",
        "Calculate WPMAPE, MAE, and BIAS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(forecasts, actuals):\n",
        "    \"\"\"\n",
        "    Calculate WMAPE, MAE, BIAS, and RMSE\n",
        "    \n",
        "    Args:\n",
        "        forecasts: array of forecasts (n_skus, horizon)\n",
        "        actuals: list of actual values\n",
        "        \n",
        "    Returns:\n",
        "        dict with metrics\n",
        "    \"\"\"\n",
        "    # Convert actuals to array\n",
        "    actuals_array = np.array(actuals)\n",
        "    \n",
        "    # Flatten for overall metrics\n",
        "    forecasts_flat = forecasts.flatten()\n",
        "    actuals_flat = actuals_array.flatten()\n",
        "    \n",
        "    # Remove any NaN or inf values\n",
        "    mask = np.isfinite(forecasts_flat) & np.isfinite(actuals_flat)\n",
        "    forecasts_flat = forecasts_flat[mask]\n",
        "    actuals_flat = actuals_flat[mask]\n",
        "    \n",
        "    # MAE (Mean Absolute Error)\n",
        "    mae = np.mean(np.abs(forecasts_flat - actuals_flat))\n",
        "    \n",
        "    # BIAS (Mean Bias - positive = over-forecast)\n",
        "    bias = np.mean(forecasts_flat - actuals_flat)\n",
        "    \n",
        "    # WMAPE (Weighted Mean Absolute Percentage Error)\n",
        "    # Weight by actual demand - PRIMARY COMPETITION METRIC\n",
        "    total_actual = np.sum(np.abs(actuals_flat))\n",
        "    if total_actual > 0:\n",
        "        wmape = np.sum(np.abs(forecasts_flat - actuals_flat)) / total_actual * 100\n",
        "    else:\n",
        "        wmape = np.nan\n",
        "    \n",
        "    # RMSE (Root Mean Squared Error - penalizes large errors more)\n",
        "    rmse = np.sqrt(np.mean((forecasts_flat - actuals_flat) ** 2))\n",
        "    \n",
        "    # Per-SKU MAE\n",
        "    per_sku_mae = []\n",
        "    for f, a in zip(forecasts, actuals_array):\n",
        "        per_sku_mae.append(np.mean(np.abs(f - a)))\n",
        "    \n",
        "    return {\n",
        "        \"MAE\": mae,\n",
        "        \"BIAS\": bias,\n",
        "        \"WMAPE\": wmape,\n",
        "        \"RMSE\": rmse,\n",
        "        \"Per_SKU_MAE_mean\": np.mean(per_sku_mae),\n",
        "        \"Per_SKU_MAE_median\": np.median(per_sku_mae),\n",
        "    }\n",
        "\n",
        "# Calculate metrics for all three methods\n",
        "print(\"Calculating metrics...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "baseline_metrics = calculate_metrics(point_forecast_baseline, actuals)\n",
        "print(\"\\\\nüìä BASELINE (TimesFM only):\")\n",
        "for metric, value in baseline_metrics.items():\n",
        "    print(f\"  {metric:20s}: {value:10.4f}\")\n",
        "\n",
        "cov_metrics = calculate_metrics(cov_forecast_array, actuals)\n",
        "print(\"\\\\nüìä ENHANCED (TimesFM + Covariates):\")\n",
        "for metric, value in cov_metrics.items():\n",
        "    print(f\"  {metric:20s}: {value:10.4f}\")\n",
        "\n",
        "xreg_metrics = calculate_metrics(xreg_forecast_array, actuals)\n",
        "print(\"\\\\nüìä REFERENCE (Linear Regression only):\")\n",
        "for metric, value in xreg_metrics.items():\n",
        "    print(f\"  {metric:20s}: {value:10.4f}\")\n",
        "\n",
        "# Calculate improvements\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"\\\\nüéØ IMPROVEMENTS (Enhanced vs Baseline):\")\n",
        "mae_improvement = (1 - cov_metrics[\"MAE\"] / baseline_metrics[\"MAE\"]) * 100\n",
        "wpmape_improvement = (1 - cov_metrics[\"WPMAPE\"] / baseline_metrics[\"WPMAPE\"]) * 100\n",
        "bias_reduction = abs(baseline_metrics[\"BIAS\"]) - abs(cov_metrics[\"BIAS\"])\n",
        "\n",
        "print(f\"  MAE improvement:     {mae_improvement:+.2f}%\")\n",
        "print(f\"  WPMAPE improvement:  {wpmape_improvement:+.2f}%\")\n",
        "print(f\"  BIAS reduction:      {bias_reduction:+.4f} units\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualize Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics comparison bar chart\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "methods = ['Baseline', 'Enhanced', 'XReg Only']\n",
        "colors = ['#FF6B6B', '#4ECDC4', '#95E1D3']\n",
        "\n",
        "# MAE\n",
        "mae_values = [baseline_metrics['MAE'], cov_metrics['MAE'], xreg_metrics['MAE']]\n",
        "axes[0].bar(methods, mae_values, color=colors)\n",
        "axes[0].set_title('Mean Absolute Error (MAE)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('MAE', fontsize=12)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# WPMAPE\n",
        "wpmape_values = [baseline_metrics['WPMAPE'], cov_metrics['WPMAPE'], xreg_metrics['WPMAPE']]\n",
        "axes[1].bar(methods, wpmape_values, color=colors)\n",
        "axes[1].set_title('Weighted MAPE (WPMAPE %)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('WPMAPE %', fontsize=12)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "\n",
        "# BIAS\n",
        "bias_values = [baseline_metrics['BIAS'], cov_metrics['BIAS'], xreg_metrics['BIAS']]\n",
        "axes[2].bar(methods, bias_values, color=colors)\n",
        "axes[2].axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
        "axes[2].set_title('BIAS (Forecast - Actual)', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('BIAS', fontsize=12)\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample forecasts for a few SKUs\n",
        "num_examples = min(6, len(inputs))\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(num_examples):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Historical data (last 20 weeks)\n",
        "    hist_data = inputs[i][-20:]\n",
        "    hist_x = range(len(hist_data))\n",
        "    ax.plot(hist_x, hist_data, 'o-', label='Historical', color='blue', alpha=0.6, linewidth=2)\n",
        "    \n",
        "    # Forecasts\n",
        "    forecast_x = range(len(hist_data), len(hist_data) + HORIZON)\n",
        "    \n",
        "    # Actuals\n",
        "    ax.plot(forecast_x, actuals[i], 's-', label='Actual', color='black', linewidth=2, markersize=8)\n",
        "    \n",
        "    # Baseline\n",
        "    ax.plot(forecast_x, point_forecast_baseline[i], '^--', label='Baseline', \n",
        "            color='#FF6B6B', linewidth=2, markersize=6)\n",
        "    \n",
        "    # Enhanced\n",
        "    ax.plot(forecast_x, cov_forecast_array[i], 'v-', label='Enhanced', \n",
        "            color='#4ECDC4', linewidth=2, markersize=6)\n",
        "    \n",
        "    # Styling\n",
        "    ax.axvline(x=len(hist_data)-0.5, color='gray', linestyle=':', alpha=0.5, linewidth=2)\n",
        "    ax.set_title(f'SKU {i+1}: Store {sku_list.iloc[i][\"Store\"]}, Product {sku_list.iloc[i][\"Product\"]}', \n",
        "                 fontsize=11, fontweight='bold')\n",
        "    ax.set_xlabel('Week', fontsize=10)\n",
        "    ax.set_ylabel('Sales Quantity', fontsize=10)\n",
        "    ax.legend(loc='best', fontsize=9)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Calculate errors for this SKU\n",
        "    mae_baseline = np.mean(np.abs(point_forecast_baseline[i] - actuals[i]))\n",
        "    mae_enhanced = np.mean(np.abs(cov_forecast_array[i] - actuals[i]))\n",
        "    improvement = (1 - mae_enhanced / mae_baseline) * 100 if mae_baseline > 0 else 0\n",
        "    \n",
        "    ax.text(0.02, 0.98, f'MAE Improvement: {improvement:+.1f}%', \n",
        "            transform=ax.transAxes, fontsize=9, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Summary\n",
        "\n",
        "### Key Findings\n",
        "\n",
        "This analysis demonstrates TimesFM 2.5 with covariates on VN2 inventory data and compares against competition benchmarks:\n",
        "\n",
        "**Model Performance:**\n",
        "- **TimesFM** provides strong zero-shot forecasting without any training\n",
        "- **Adding covariates** (product hierarchy, temporal features) improves accuracy\n",
        "- **Seasonal Moving Average** (official benchmark) performs well with clear seasonality\n",
        "- **Model-based routing** (SES/Croston/TBATS) adapts to SKU characteristics\n",
        "- **Linear regression alone** (XReg) is insufficient for complex demand patterns\n",
        "\n",
        "**Methods Compared:**\n",
        "1. **Seasonal MA (Official Benchmark)**: 8-week moving average with multiplicative seasonal adjustment\n",
        "2. **SES/Croston/TBATS (Model-Routed)**: Per-SKU routing based on intermittency and seasonality\n",
        "3. **TimesFM (Baseline)**: Pre-trained foundation model, zero-shot\n",
        "4. **TimesFM + Covariates (Enhanced)**: TimesFM with in-context regression\n",
        "5. **XReg Only (Reference)**: Pure linear regression on covariates\n",
        "\n",
        "**Covariates Used:**\n",
        "- **Static**: Store, ProductGroup, Department, historical mean/CV\n",
        "- **Dynamic**: Month, Week of Year, Quarter, Week Index\n",
        "\n",
        "**Evaluation Metrics:**\n",
        "- **WPMAPE**: Weighted by actual demand (accounts for volume differences)\n",
        "- **MAE**: Average absolute error across all forecasts\n",
        "- **BIAS**: Tendency to over/under forecast (positive = over-forecast)\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Expand to all SKUs**: Test on full 600 SKU dataset\n",
        "2. **Add more covariates**: Promotions, holidays, weather (if available)\n",
        "3. **Tune hyperparameters**: Context length, ridge penalty, xreg_mode\n",
        "4. **Segment-specific models**: Different models for ABC/XYZ categories\n",
        "5. **Ensemble methods**: Combine TimesFM with traditional methods\n",
        "6. **Inventory policy**: Apply forecasts to base-stock policy for actual submissions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Benchmark Methods\n",
        "\n",
        "Let's add the competition benchmarks for comparison:\n",
        "- **Official Benchmark**: 8-week seasonal moving average\n",
        "- **Model-based Routing**: SES/Croston/TBATS per-SKU selection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1 Official Benchmark: 8-Week Seasonal Moving Average\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def forecast_seasonal_ma(sales_data, horizon=3, window=8):\n",
        "    \"\"\"\n",
        "    8-week seasonal moving average forecast (official benchmark)\n",
        "    \n",
        "    Args:\n",
        "        sales_data: pd.DataFrame with datetime columns (as DatetimeIndex or convertible)\n",
        "        horizon: forecast horizon in weeks\n",
        "        window: moving average window in weeks\n",
        "        \n",
        "    Returns:\n",
        "        np.ndarray of forecasts\n",
        "    \"\"\"\n",
        "    # Ensure columns are DatetimeIndex\n",
        "    if not isinstance(sales_data.columns, pd.DatetimeIndex):\n",
        "        sales_data = sales_data.copy()\n",
        "        sales_data.columns = pd.to_datetime(sales_data.columns)\n",
        "    \n",
        "    # Step 1: Compute seasonal factors (multiplicative)\n",
        "    season = sales_data.mean(axis=0).rename(\"Demand\").to_frame()\n",
        "    # Now season.index should be DatetimeIndex\n",
        "    season[\"Week Number\"] = season.index.isocalendar().week\n",
        "    season = season.groupby(\"Week Number\").mean()\n",
        "    season = season / season.mean()  # Normalize to 1\n",
        "    \n",
        "    # Step 2: Un-seasonalize demand\n",
        "    sales_weeks = sales_data.columns.isocalendar().week\n",
        "    sales_no_season = sales_data / (season.loc[sales_weeks.values]).values.reshape(-1)\n",
        "    \n",
        "    # Step 3: 8-week moving average on unseasonalized data\n",
        "    base_forecast = sales_no_season.iloc[:, -window:].mean(axis=1)\n",
        "    \n",
        "    # Step 4: Re-seasonalize for future periods (exactly horizon steps)\n",
        "    inferred_freq = getattr(sales_data.columns, 'freqstr', None) or pd.infer_freq(sales_data.columns)\n",
        "    if inferred_freq is None:\n",
        "        inferred_freq = 'W'\n",
        "    start_date = sales_data.columns[-1] + pd.tseries.frequencies.to_offset(inferred_freq)\n",
        "    f_periods = pd.date_range(start=start_date, periods=horizon, freq=inferred_freq)\n",
        "    forecast = pd.DataFrame(\n",
        "        data=base_forecast.values.reshape(-1, 1).repeat(len(f_periods), axis=1),\n",
        "        columns=f_periods,\n",
        "        index=sales_data.index\n",
        "    )\n",
        "    forecast = forecast * (season.loc[f_periods.isocalendar().week.values]).values.reshape(-1)\n",
        "    \n",
        "    return forecast.values\n",
        "\n",
        "print(\"Computing seasonal moving average forecasts...\")\n",
        "\n",
        "# Prepare wide format for seasonal MA - filter to match sku_list exactly\n",
        "sales_wide_filtered = sales_wide.copy()\n",
        "\n",
        "# Create a sorted tuple list to ensure order matches sku_list\n",
        "sku_tuples = list(sku_list.apply(tuple, axis=1))\n",
        "\n",
        "# Filter and sort to match sku_list order\n",
        "sales_wide_filtered = sales_wide_filtered[\n",
        "    sales_wide_filtered[[\"Store\", \"Product\"]].apply(tuple, axis=1).isin(sku_tuples)\n",
        "].reset_index(drop=True)\n",
        "\n",
        "# Sort to match sku_list order\n",
        "sales_wide_filtered['_sort_key'] = sales_wide_filtered[[\"Store\", \"Product\"]].apply(\n",
        "    lambda x: sku_tuples.index((x[\"Store\"], x[\"Product\"])) if (x[\"Store\"], x[\"Product\"]) in sku_tuples else -1,\n",
        "    axis=1\n",
        ")\n",
        "sales_wide_filtered = sales_wide_filtered.sort_values('_sort_key').drop('_sort_key', axis=1).reset_index(drop=True)\n",
        "\n",
        "# Convert to datetime columns\n",
        "date_cols = [col for col in sales_wide_filtered.columns if col not in [\"Store\", \"Product\"]]\n",
        "sales_wide_filtered.columns = [\"Store\", \"Product\"] + [pd.to_datetime(col) for col in date_cols]\n",
        "\n",
        "# Set index and get the relevant context window\n",
        "sales_wide_filtered = sales_wide_filtered.set_index([\"Store\", \"Product\"])\n",
        "context_end_date = pd.to_datetime(date_cols[TEST_START_WEEK - 1])\n",
        "context_sales = sales_wide_filtered.loc[:, :context_end_date]\n",
        "\n",
        "# Forecast for all SKUs at once\n",
        "seasonal_ma_forecast = forecast_seasonal_ma(context_sales, horizon=HORIZON, window=8)\n",
        "\n",
        "print(f\"Seasonal MA forecast shape: {seasonal_ma_forecast.shape}\")\n",
        "print(f\"Expected shape: ({len(sku_list)}, {HORIZON})\")\n",
        "print(f\"Actuals shape: ({len(actuals)}, {HORIZON})\")\n",
        "seasonal_ma_forecast[:5]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.2 Model-Based Routing: SES/Croston/TBATS\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check required packages\n",
        "try:\n",
        "    import statsmodels\n",
        "    print(\"‚úì statsmodels available:\", statsmodels.__version__)\n",
        "except ImportError:\n",
        "    print(\"‚ö† statsmodels not available - install with: pip install statsmodels\")\n",
        "\n",
        "# TBATS has numpy 2.x incompatibility, so we make it optional\n",
        "TBATS_AVAILABLE = False\n",
        "try:\n",
        "    from tbats import TBATS as _TBATS_check\n",
        "    TBATS_AVAILABLE = True\n",
        "    print(\"‚úì TBATS available\")\n",
        "except (ImportError, ValueError) as e:\n",
        "    print(f\"‚ö† TBATS not available (numpy incompatibility) - will use SES/Croston only\")\n",
        "    print(f\"  Error: {type(e).__name__}: {str(e)[:100]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import forecasting libraries\n",
        "from statsmodels.tsa.holtwinters import SimpleExpSmoothing, ExponentialSmoothing\n",
        "from statsmodels.tsa.seasonal import STL\n",
        "from collections import Counter\n",
        "\n",
        "# Try to import TBATS (may fail due to numpy incompatibility)\n",
        "try:\n",
        "    from tbats import TBATS\n",
        "    TBATS_AVAILABLE = True\n",
        "except (ImportError, ValueError):\n",
        "    TBATS_AVAILABLE = False\n",
        "    print(\"‚ö† TBATS import failed - will skip TBATS forecasts\")\n",
        "\n",
        "\n",
        "def fc_ses(train: pd.Series, horizon: int) -> np.ndarray:\n",
        "    \"\"\"Simple Exponential Smoothing\"\"\"\n",
        "    if len(train) < 2:\n",
        "        return np.repeat(train.iloc[-1] if len(train) else 0.0, horizon)\n",
        "    model = SimpleExpSmoothing(train.astype(float)).fit(optimized=True)\n",
        "    return model.forecast(horizon).values\n",
        "\n",
        "\n",
        "def fc_croston_sba(train: pd.Series, horizon: int) -> np.ndarray:\n",
        "    \"\"\"Croston's method with Syntetos-Boylan Adjustment (for intermittent demand)\"\"\"\n",
        "    y = train.values.astype(float)\n",
        "    alpha = 0.1\n",
        "    z, p = 0.0, 0.0\n",
        "    q = 0\n",
        "    for v in y:\n",
        "        if v > 0:\n",
        "            if z == 0:\n",
        "                z = v\n",
        "                p = 1\n",
        "            else:\n",
        "                z = alpha * v + (1 - alpha) * z\n",
        "                p = alpha * (q + 1) + (1 - alpha) * p\n",
        "            q = 0\n",
        "        else:\n",
        "            q += 1\n",
        "    rate = z / p if p > 0 else 0.0\n",
        "    return np.repeat(rate * (1 - alpha / 2), horizon)\n",
        "\n",
        "\n",
        "def fc_holt_winters(train: pd.Series, horizon: int, seasonal_periods: int = 52) -> np.ndarray:\n",
        "    \"\"\"Holt-Winters seasonal exponential smoothing\"\"\"\n",
        "    y = train.astype(float)\n",
        "    if len(y) < 2 * seasonal_periods:\n",
        "        return fc_ses(train, horizon)\n",
        "    \n",
        "    try:\n",
        "        # Try multiplicative seasonality first (better for series with level changes)\n",
        "        model = ExponentialSmoothing(\n",
        "            y, \n",
        "            seasonal_periods=seasonal_periods, \n",
        "            trend='add', \n",
        "            seasonal='mul',\n",
        "            initialization_method='estimated'\n",
        "        ).fit()\n",
        "        fc = model.forecast(horizon)\n",
        "        return np.asarray(fc, dtype=float)\n",
        "    except Exception:\n",
        "        try:\n",
        "            # Fall back to additive seasonality\n",
        "            model = ExponentialSmoothing(\n",
        "                y, \n",
        "                seasonal_periods=seasonal_periods, \n",
        "                trend='add', \n",
        "                seasonal='add',\n",
        "                initialization_method='estimated'\n",
        "            ).fit()\n",
        "            fc = model.forecast(horizon)\n",
        "            return np.asarray(fc, dtype=float)\n",
        "        except Exception:\n",
        "            # If both fail, use SES\n",
        "            return fc_ses(train, horizon)\n",
        "\n",
        "\n",
        "def fc_tbats(train: pd.Series, horizon: int) -> np.ndarray:\n",
        "    \"\"\"TBATS for seasonal time series with cascading fallbacks\"\"\"\n",
        "    y = train.astype(float).values\n",
        "    if len(y) < 10:\n",
        "        return np.repeat(train.iloc[-1] if len(train) else 0.0, horizon)\n",
        "\n",
        "    # Try in-process TBATS first if available\n",
        "    if TBATS_AVAILABLE:\n",
        "        try:\n",
        "            estimator = TBATS(use_arma_errors=False, use_box_cox=False, show_warnings=False)\n",
        "            model = estimator.fit(y)\n",
        "            fc = model.forecast(steps=horizon)\n",
        "            return np.asarray(fc, dtype=float)\n",
        "        except Exception:\n",
        "            pass  # Fall back to external\n",
        "\n",
        "    # External TBATS via dedicated venv (to avoid numpy/JAX conflicts)\n",
        "    import subprocess, json, tempfile, os\n",
        "    TBATS_PY = \"/Users/senoni/noni/timesfm/.venv_tbats/bin/python\"\n",
        "    try:\n",
        "        with tempfile.TemporaryDirectory() as d:\n",
        "            inp = os.path.join(d, \"y.csv\")\n",
        "            out = os.path.join(d, \"fc.json\")\n",
        "            pd.Series(y).to_csv(inp, index=False, header=False)\n",
        "            code = f\"\"\"\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import json, pandas as pd\n",
        "from tbats import TBATS\n",
        "y = pd.read_csv(r'{inp}', header=None)[0].astype(float).values\n",
        "est = TBATS(use_arma_errors=False, use_box_cox=False, show_warnings=False)\n",
        "mdl = est.fit(y)\n",
        "fc = mdl.forecast(steps={horizon})\n",
        "json.dump(fc.tolist(), open(r'{out}', 'w'))\n",
        "\"\"\"\n",
        "            subprocess.run([TBATS_PY, \"-c\", code], check=True, capture_output=True)\n",
        "            return np.array(json.load(open(out)))\n",
        "    except Exception:\n",
        "        pass  # Fall back to Holt-Winters\n",
        "    \n",
        "    # Fallback 1: Try Holt-Winters (seasonal ETS) - appropriate for seasonal series\n",
        "    try:\n",
        "        return fc_holt_winters(pd.Series(y), horizon, seasonal_periods=52)\n",
        "    except Exception:\n",
        "        pass\n",
        "    \n",
        "    # Final fallback: SES (non-seasonal)\n",
        "    return fc_ses(pd.Series(y), horizon)\n",
        "\n",
        "print(f\"‚úì Forecasting functions defined (TBATS: {'available' if TBATS_AVAILABLE else 'external fallback enabled'})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Routing thresholds\n",
        "ZERO_FRAC_THR = 0.5\n",
        "LOW_MEAN_THR = 1.5\n",
        "SEASON_STRENGTH_THR = 0.3\n",
        "MIN_LEN_TBATS = 60\n",
        "\n",
        "if TBATS_AVAILABLE:\n",
        "    print(\"Computing model-routed forecasts (SES/Croston/TBATS in-process)...\")\n",
        "else:\n",
        "    print(\"Computing model-routed forecasts (SES/Croston/TBATS external)...\")\n",
        "\n",
        "routed_forecasts = []\n",
        "model_used = []\n",
        "tbats_external_count = 0  # Track external TBATS usage\n",
        "\n",
        "for idx, row in sku_list.iterrows():\n",
        "    store, product = row[\"Store\"], row[\"Product\"]\n",
        "    \n",
        "    # Get sales history for this SKU\n",
        "    sku_data = sales_long[\n",
        "        (sales_long[\"Store\"] == store) & \n",
        "        (sales_long[\"Product\"] == product)\n",
        "    ].sort_values(\"date\").reset_index(drop=True)\n",
        "    \n",
        "    # Use training data\n",
        "    train_data = sku_data.iloc[:TEST_START_WEEK].copy()\n",
        "    y = train_data[\"sales_qty\"].astype(float)\n",
        "    \n",
        "    # Compute routing features\n",
        "    zero_frac = (y == 0).mean()\n",
        "    mean_y = y.mean()\n",
        "    \n",
        "    # Seasonal strength (STL)\n",
        "    seas_strength = 0.0\n",
        "    if len(y) >= MIN_LEN_TBATS:\n",
        "        try:\n",
        "            y_series = pd.Series(y.values, index=pd.date_range(start=\"2021-01-01\", periods=len(y), freq=\"W\"))\n",
        "            seas_strength = STL(y_series, period=52, robust=True).fit().seasonal.var() / (y.var() + 1e-9)\n",
        "        except Exception:\n",
        "            seas_strength = 0.0\n",
        "    \n",
        "    # Route to model\n",
        "    if zero_frac >= ZERO_FRAC_THR and mean_y < LOW_MEAN_THR:\n",
        "        # Intermittent demand ‚Üí Croston\n",
        "        model_name = \"croston\"\n",
        "        fc = fc_croston_sba(y, HORIZON)\n",
        "    elif seas_strength >= SEASON_STRENGTH_THR and mean_y >= 3 and len(y) >= MIN_LEN_TBATS:\n",
        "        # Seasonal ‚Üí TBATS\n",
        "        model_name = \"tbats\"\n",
        "        fc = fc_tbats(y, HORIZON)\n",
        "    else:\n",
        "        # Default ‚Üí SES\n",
        "        model_name = \"ses\"\n",
        "        fc = fc_ses(y, HORIZON)\n",
        "    \n",
        "    routed_forecasts.append(fc)\n",
        "    model_used.append(model_name)\n",
        "\n",
        "routed_forecast_array = np.array(routed_forecasts)\n",
        "\n",
        "print(f\"\\nRouted forecast shape: {routed_forecast_array.shape}\")\n",
        "print(f\"\\nModel distribution:\")\n",
        "model_counts = Counter(model_used)\n",
        "for model, count in sorted(model_counts.items()):\n",
        "    print(f\"  {model:15s}: {count:3d} SKUs ({count/len(model_used)*100:5.1f}%)\")\n",
        "\n",
        "if not TBATS_AVAILABLE and 'tbats' in model_counts:\n",
        "    print(f\"\\n‚úì External TBATS successfully used for {model_counts['tbats']} SKUs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.3 Evaluate All Methods\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate metrics for benchmark methods\n",
        "print(\"Calculating metrics for all methods...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "seasonal_ma_metrics = calculate_metrics(seasonal_ma_forecast, actuals)\n",
        "print(\"\\nüìä BENCHMARK 1: Seasonal Moving Average (8-week):\")\n",
        "for metric, value in seasonal_ma_metrics.items():\n",
        "    print(f\"  {metric:20s}: {value:10.4f}\")\n",
        "\n",
        "routed_metrics = calculate_metrics(routed_forecast_array, actuals)\n",
        "print(\"\\nüìä BENCHMARK 2: Model-Routed (SES/Croston/TBATS):\")\n",
        "for metric, value in routed_metrics.items():\n",
        "    print(f\"  {metric:20s}: {value:10.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\\nüéØ COMPARISON SUMMARY:\")\n",
        "print(f\"{'Method':<30} {'MAE':>10} {'WPMAPE':>10} {'BIAS':>10}\")\n",
        "print(\"-\"*70)\n",
        "print(f\"{'Seasonal MA (Official)':<30} {seasonal_ma_metrics['MAE']:>10.4f} {seasonal_ma_metrics['WPMAPE']:>10.2f} {seasonal_ma_metrics['BIAS']:>10.4f}\")\n",
        "print(f\"{'SES/Croston/TBATS (Routed)':<30} {routed_metrics['MAE']:>10.4f} {routed_metrics['WPMAPE']:>10.2f} {routed_metrics['BIAS']:>10.4f}\")\n",
        "print(f\"{'TimesFM (Baseline)':<30} {baseline_metrics['MAE']:>10.4f} {baseline_metrics['WPMAPE']:>10.2f} {baseline_metrics['BIAS']:>10.4f}\")\n",
        "print(f\"{'TimesFM + Covariates (Enhanced)':<30} {cov_metrics['MAE']:>10.4f} {cov_metrics['WPMAPE']:>10.2f} {cov_metrics['BIAS']:>10.4f}\")\n",
        "print(f\"{'XReg Only (Reference)':<30} {xreg_metrics['MAE']:>10.4f} {xreg_metrics['WPMAPE']:>10.2f} {xreg_metrics['BIAS']:>10.4f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Improvements vs best benchmark\n",
        "best_benchmark_mae = min(seasonal_ma_metrics['MAE'], routed_metrics['MAE'])\n",
        "best_benchmark_wpmape = min(seasonal_ma_metrics['WPMAPE'], routed_metrics['WPMAPE'])\n",
        "\n",
        "print(f\"\\nüèÜ TimesFM + Covariates vs Best Benchmark:\")\n",
        "mae_improvement = (1 - cov_metrics[\"MAE\"] / best_benchmark_mae) * 100\n",
        "wpmape_improvement = (1 - cov_metrics[\"WPMAPE\"] / best_benchmark_wpmape) * 100\n",
        "print(f\"  MAE improvement:     {mae_improvement:+.2f}%\")\n",
        "print(f\"  WPMAPE improvement:  {wpmape_improvement:+.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.4 Visual Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated metrics comparison bar chart\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "methods = ['Seasonal MA', 'SES/Croston/TBATS', 'TimesFM', 'TimesFM+Cov', 'XReg']\n",
        "colors = ['#95E1D3', '#FFB6B9', '#FF6B6B', '#4ECDC4', '#C7CEEA']\n",
        "\n",
        "# MAE\n",
        "mae_values = [\n",
        "    seasonal_ma_metrics['MAE'], \n",
        "    routed_metrics['MAE'],\n",
        "    baseline_metrics['MAE'], \n",
        "    cov_metrics['MAE'], \n",
        "    xreg_metrics['MAE']\n",
        "]\n",
        "bars = axes[0].bar(methods, mae_values, color=colors)\n",
        "axes[0].set_title('Mean Absolute Error (MAE)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('MAE', fontsize=12)\n",
        "axes[0].grid(axis='y', alpha=0.3)\n",
        "axes[0].tick_params(axis='x', rotation=15)\n",
        "# Highlight best\n",
        "best_idx = mae_values.index(min(mae_values))\n",
        "bars[best_idx].set_edgecolor('black')\n",
        "bars[best_idx].set_linewidth(3)\n",
        "\n",
        "# WPMAPE\n",
        "wpmape_values = [\n",
        "    seasonal_ma_metrics['WPMAPE'],\n",
        "    routed_metrics['WPMAPE'], \n",
        "    baseline_metrics['WPMAPE'], \n",
        "    cov_metrics['WPMAPE'], \n",
        "    xreg_metrics['WPMAPE']\n",
        "]\n",
        "bars = axes[1].bar(methods, wpmape_values, color=colors)\n",
        "axes[1].set_title('Weighted MAPE (WPMAPE %)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('WPMAPE %', fontsize=12)\n",
        "axes[1].grid(axis='y', alpha=0.3)\n",
        "axes[1].tick_params(axis='x', rotation=15)\n",
        "# Highlight best\n",
        "best_idx = wpmape_values.index(min(wpmape_values))\n",
        "bars[best_idx].set_edgecolor('black')\n",
        "bars[best_idx].set_linewidth(3)\n",
        "\n",
        "# BIAS\n",
        "bias_values = [\n",
        "    seasonal_ma_metrics['BIAS'],\n",
        "    routed_metrics['BIAS'],\n",
        "    baseline_metrics['BIAS'], \n",
        "    cov_metrics['BIAS'], \n",
        "    xreg_metrics['BIAS']\n",
        "]\n",
        "bars = axes[2].bar(methods, bias_values, color=colors)\n",
        "axes[2].axhline(y=0, color='black', linestyle='--', alpha=0.5, linewidth=1)\n",
        "axes[2].set_title('BIAS (Forecast - Actual)', fontsize=14, fontweight='bold')\n",
        "axes[2].set_ylabel('BIAS', fontsize=12)\n",
        "axes[2].grid(axis='y', alpha=0.3)\n",
        "axes[2].tick_params(axis='x', rotation=15)\n",
        "# Highlight closest to zero\n",
        "best_idx = bias_values.index(min(bias_values, key=abs))\n",
        "bars[best_idx].set_edgecolor('black')\n",
        "bars[best_idx].set_linewidth(3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample forecasts comparison with ALL methods\n",
        "num_examples = min(4, len(inputs))\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i in range(num_examples):\n",
        "    ax = axes[i]\n",
        "    \n",
        "    # Historical data (last 20 weeks)\n",
        "    hist_data = inputs[i][-20:]\n",
        "    hist_x = range(len(hist_data))\n",
        "    ax.plot(hist_x, hist_data, 'o-', label='Historical', color='blue', alpha=0.6, linewidth=2)\n",
        "    \n",
        "    # Forecasts\n",
        "    forecast_x = range(len(hist_data), len(hist_data) + HORIZON)\n",
        "    \n",
        "    # Actuals\n",
        "    ax.plot(forecast_x, actuals[i], 's-', label='Actual', color='black', linewidth=3, markersize=8, zorder=10)\n",
        "    \n",
        "    # All forecasts\n",
        "    ax.plot(forecast_x, seasonal_ma_forecast[i], 'D--', label='Seasonal MA', \n",
        "            color='#95E1D3', linewidth=2, markersize=5, alpha=0.7)\n",
        "    ax.plot(forecast_x, routed_forecast_array[i], '^--', label='SES/Croston/TBATS', \n",
        "            color='#FFB6B9', linewidth=2, markersize=5, alpha=0.7)\n",
        "    ax.plot(forecast_x, point_forecast_baseline[i], 'v--', label='TimesFM', \n",
        "            color='#FF6B6B', linewidth=2, markersize=5, alpha=0.7)\n",
        "    ax.plot(forecast_x, cov_forecast_array[i], '*-', label='TimesFM+Cov', \n",
        "            color='#4ECDC4', linewidth=2.5, markersize=8)\n",
        "    \n",
        "    # Styling\n",
        "    ax.axvline(x=len(hist_data)-0.5, color='gray', linestyle=':', alpha=0.5, linewidth=2)\n",
        "    ax.set_title(f'SKU {i+1}: Store {sku_list.iloc[i][\"Store\"]}, Product {sku_list.iloc[i][\"Product\"]}', \n",
        "                 fontsize=11, fontweight='bold')\n",
        "    ax.set_xlabel('Week', fontsize=10)\n",
        "    ax.set_ylabel('Sales Quantity', fontsize=10)\n",
        "    ax.legend(loc='best', fontsize=8)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Calculate errors for this SKU\n",
        "    mae_seasonal = np.mean(np.abs(seasonal_ma_forecast[i] - actuals[i]))\n",
        "    mae_routed = np.mean(np.abs(routed_forecast_array[i] - actuals[i]))\n",
        "    mae_baseline = np.mean(np.abs(point_forecast_baseline[i] - actuals[i]))\n",
        "    mae_enhanced = np.mean(np.abs(cov_forecast_array[i] - actuals[i]))\n",
        "    \n",
        "    best_mae = min(mae_seasonal, mae_routed, mae_baseline, mae_enhanced)\n",
        "    best_method = ['Seasonal MA', 'Routed', 'TimesFM', 'TimesFM+Cov'][\n",
        "        [mae_seasonal, mae_routed, mae_baseline, mae_enhanced].index(best_mae)\n",
        "    ]\n",
        "    \n",
        "    ax.text(0.02, 0.98, f'Best: {best_method} (MAE={best_mae:.2f})', \n",
        "            transform=ax.transAxes, fontsize=9, verticalalignment='top',\n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
